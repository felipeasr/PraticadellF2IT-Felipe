{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10a2d32aeb3b4ad38394719d69753daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d1a4a64a3cd49de954b533760f2b3dd",
              "IPY_MODEL_350c90a110324069b4aaca6f474f2bdd",
              "IPY_MODEL_ffbda6118e0b44cb9948a058c089f24d"
            ],
            "layout": "IPY_MODEL_121cd40b2c36481c816a7b663fa5e952"
          }
        },
        "9d1a4a64a3cd49de954b533760f2b3dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46484500f2224dfb90f700250a00c38a",
            "placeholder": "​",
            "style": "IPY_MODEL_1028cd2f363241fa9214ba213773a90a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "350c90a110324069b4aaca6f474f2bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b4d6c891804453b38f7e10cfeb179d",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19600079cacc4d45a34e19f48a11299e",
            "value": 8
          }
        },
        "ffbda6118e0b44cb9948a058c089f24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a879553094e45a1b6d88247387b93f0",
            "placeholder": "​",
            "style": "IPY_MODEL_b5ded8e47b2941df9eaa30955637dcec",
            "value": " 8/8 [01:09&lt;00:00,  7.61s/it]"
          }
        },
        "121cd40b2c36481c816a7b663fa5e952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46484500f2224dfb90f700250a00c38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1028cd2f363241fa9214ba213773a90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b4d6c891804453b38f7e10cfeb179d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19600079cacc4d45a34e19f48a11299e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a879553094e45a1b6d88247387b93f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ded8e47b2941df9eaa30955637dcec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Felipe Ribeiro\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FrpPwZ2mzmWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "RbjJwNqJzvGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MjCcRxpAlKxJ"
      },
      "outputs": [],
      "source": [
        "# Instalação das Bibliotecas\n",
        "\n",
        "!pip install transformers accelerate bitsandbytes PyPDF2 --quiet\n",
        "!pip install langchain sentence-transformers faiss-cpu --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações e Carregamento do Modelo\n",
        "import torch\n",
        "import json\n",
        "import PyPDF2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Configuração para carregar o modelo com quantização de 4 bits\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Nome do modelo no Hugging Face\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Carrega o tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Carrega o modelo com a configuração de quantização\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\", # Mapeia o modelo automaticamente para a GPU\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "10a2d32aeb3b4ad38394719d69753daa",
            "9d1a4a64a3cd49de954b533760f2b3dd",
            "350c90a110324069b4aaca6f474f2bdd",
            "ffbda6118e0b44cb9948a058c089f24d",
            "121cd40b2c36481c816a7b663fa5e952",
            "46484500f2224dfb90f700250a00c38a",
            "1028cd2f363241fa9214ba213773a90a",
            "05b4d6c891804453b38f7e10cfeb179d",
            "19600079cacc4d45a34e19f48a11299e",
            "6a879553094e45a1b6d88247387b93f0",
            "b5ded8e47b2941df9eaa30955637dcec"
          ]
        },
        "id": "wE4dA26MlhDL",
        "outputId": "d61125be-2014-4f01-c55f-439a89afb31e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10a2d32aeb3b4ad38394719d69753daa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import re\n",
        "\n",
        "def limpar_texto(texto):\n",
        "    \"\"\"Remove quebras de linha excessivas e outros artefatos.\"\"\"\n",
        "    # Substitui múltiplos espaços/quebras de linha por um único espaço\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.strip()\n",
        "\n",
        "def criar_indice_pesquisavel(texto_pdf):\n",
        "    \"\"\"\n",
        "    Divide o texto do PDF em pedaços, cria embeddings e retorna um índice FAISS.\n",
        "    \"\"\"\n",
        "    # 1. Dividir o texto em pedaços menores (chunks)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,  # Tamanho de cada pedaço em caracteres\n",
        "        chunk_overlap=100, # Sobreposição entre pedaços para não perder contexto\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(texto_pdf)\n",
        "\n",
        "    # 2. Criar Embeddings (transformar texto em vetores)\n",
        "    # Usaremos um modelo mais leve, otimizado para português, para esta tarefa.\n",
        "    model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "    # 3. Criar o índice FAISS a partir dos chunks e embeddings\n",
        "    # FAISS é uma biblioteca super eficiente para busca de similaridade.\n",
        "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
        "    return vector_store\n",
        "\n",
        "def buscar_contexto_relevante(indice, reivindicacao, k=3):\n",
        "    docs_relevantes = indice.similarity_search(reivindicacao, k=k)\n",
        "    # Concatena o conteúdo dos documentos encontrados\n",
        "    contexto = \"\\n---\\n\".join([doc.page_content for doc in docs_relevantes])\n",
        "    return contexto"
      ],
      "metadata": {
        "id": "lpheyWqgmuJy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt + trata o json\n",
        "def analisar_reivindicacao_com_contexto(reivindicacao, contexto):\n",
        "    \"\"\"\n",
        "    Usa o LLM para analisar UMA ÚNICA reivindicação com base em um contexto específico.\n",
        "    \"\"\"\n",
        "    # Prompt aprimorado com a técnica \"few-shot\", dando um exemplo claro do que esperamos.\n",
        "    prompt = f\"\"\"<|system|>\n",
        "Você é um especialista em análise jurídica. Sua tarefa é avaliar se a 'REIVINDICAÇÃO' é suportada pelo 'CONTEXTO' fornecido.\n",
        "Responda APENAS com um único objeto JSON válido. Não adicione nenhuma explicação ou texto antes ou depois do objeto JSON.\n",
        "\n",
        "Exemplo de resposta esperada:\n",
        "{{\n",
        "  \"label\": \"Incorreta\",\n",
        "  \"evidence\": \"A justificativa para a incorreção, baseada estritamente no contexto.\"\n",
        "}}\n",
        "</s>\n",
        "<|user|>\n",
        "'CONTEXTO':\n",
        "---\n",
        "{contexto}\n",
        "---\n",
        "\n",
        "'REIVINDICAÇÃO':\n",
        "\"{reivindicacao}\"\n",
        "\n",
        "Gere o objeto JSON para a reivindicação acima, baseando-se estritamente no contexto fornecido.</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    # Prepara a entrada para o modelo\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=False).to(\"cuda\")\n",
        "\n",
        "    # Gera a resposta do modelo\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300, # Aumentado um pouco para garantir que a 'evidence' caiba\n",
        "        temperature=0.0,    # Temperatura 0.0 para respostas mais diretas e menos criativas\n",
        "        do_sample=False,    # Desativa a amostragem para respostas mais determinísticas\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decodifica a resposta completa\n",
        "    resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Lógica de parsing inteligente para extrair o JSON\n",
        "    try:\n",
        "        # Pega apenas o que vem depois da tag do assistente\n",
        "        resposta_assistente = resposta_completa.split(\"<|assistant|>\")[1].strip()\n",
        "\n",
        "        # Procura pelo primeiro '{' e o último '}' para extrair o bloco JSON\n",
        "        match = re.search(r'\\{.*\\}', resposta_assistente, re.DOTALL)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            resultado_json = json.loads(json_str)\n",
        "            return resultado_json\n",
        "        else:\n",
        "            # Se não encontrar um JSON, registra o erro\n",
        "            print(f\"ERRO: Bloco JSON não encontrado na saída do modelo para a reivindicação: '{reivindicacao}'\")\n",
        "            print(f\"Saída do modelo: {resposta_assistente}\")\n",
        "            return {\"label\": \"Erro\", \"evidence\": \"Bloco JSON não encontrado na resposta do modelo.\"}\n",
        "\n",
        "    except (json.JSONDecodeError, IndexError) as e:\n",
        "        print(f\"ERRO ao decodificar JSON para a reivindicação: '{reivindicacao}'\")\n",
        "        print(f\"Saída do modelo: {resposta_completa}\")\n",
        "        return {\"label\": \"Erro\", \"evidence\": f\"Falha ao processar a resposta do modelo: {e}\"}"
      ],
      "metadata": {
        "id": "a6DB8tTqnNX6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ler o arquivos\n",
        "def ler_pdf(caminho_arquivo):\n",
        "    \"\"\"Lê o texto de um arquivo PDF.\"\"\"\n",
        "    texto = \"\"\n",
        "    with open(caminho_arquivo, 'rb') as f:\n",
        "        leitor = PyPDF2.PdfReader(f)\n",
        "        for pagina in leitor.pages:\n",
        "            texto_pagina = pagina.extract_text()\n",
        "            if texto_pagina:\n",
        "                texto += texto_pagina\n",
        "    return texto\n",
        "\n",
        "def ler_txt(caminho_arquivo):\n",
        "    \"\"\"Lê o texto de um arquivo TXT.\"\"\"\n",
        "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "# --- Definição dos arquivos ---\n",
        "arquivos_para_analisar = [\n",
        "    {\n",
        "        \"doc_path\": \"Acórdão 733 de 2025 Plenário.pdf\",\n",
        "        \"resumo_path\": \"Acórdão 733-2025 resumos.txt\",\n",
        "        \"doc_name\": \"Acórdão 733 de 2025 Plenário\",\n",
        "        \"summary_id\": 1\n",
        "    },\n",
        "    {\n",
        "        \"doc_path\": \"Acórdão 764 de 2025 Plenário.pdf\",\n",
        "        \"resumo_path\": \"Acórdão 764-2025 resumos.txt\",\n",
        "        \"doc_name\": \"Acórdão 764 de 2025 Plenário\",\n",
        "        \"summary_id\": 2\n",
        "    }\n",
        "]\n",
        "\n",
        "analise_final = []\n",
        "\n",
        "print(\"Iniciando a análise dos documentos com a nova estratégia...\")\n",
        "\n",
        "# realizar a análise\n",
        "for item in arquivos_para_analisar:\n",
        "    print(f\"\\n--- Processando: {item['doc_name']} ---\")\n",
        "    try:\n",
        "        # 1. Ler e limpar os textos\n",
        "        texto_acordao_bruto = ler_pdf(item['doc_path'])\n",
        "        texto_acordao = limpar_texto(texto_acordao_bruto)\n",
        "        texto_resumo = ler_txt(item['resumo_path'])\n",
        "\n",
        "        # 2. Criar o índice pesquisável para o acórdão\n",
        "        print(\"Criando índice de busca para o documento...\")\n",
        "        indice_acordao = criar_indice_pesquisavel(texto_acordao)\n",
        "        print(\"Índice criado com sucesso.\")\n",
        "\n",
        "        # 3. Dividir o resumo em reivindicações, ignorando títulos/linhas curtas\n",
        "        reivindicacoes = [\n",
        "            r.strip() for r in texto_resumo.split('\\n')\n",
        "            if r.strip() and len(r.strip().split()) > 3 # <-- FILTRO MELHORADO AQUI\n",
        "        ]\n",
        "        print(f\"Encontradas {len(reivindicacoes)} reivindicações válidas no resumo.\")\n",
        "\n",
        "        # 4. Analisar cada reivindicação individualmente\n",
        "        for i, claim_text in enumerate(reivindicacoes):\n",
        "            print(f\"Analisando reivindicação {i+1}/{len(reivindicacoes)}: '{claim_text[:50]}...'\")\n",
        "\n",
        "            # 4.1. Buscar contexto relevante no índice\n",
        "            contexto = buscar_contexto_relevante(indice_acordao, claim_text)\n",
        "\n",
        "            # 4.2. Chamar o LLM com a reivindicação e o contexto focado\n",
        "            resultado_analise = analisar_reivindicacao_com_contexto(claim_text, contexto)\n",
        "\n",
        "            # 4.3. Montar o objeto JSON final para esta reivindicação\n",
        "            if resultado_analise:\n",
        "                resultado_final_claim = {\n",
        "                    \"doc_name\": item['doc_name'],\n",
        "                    \"claim_text\": claim_text,\n",
        "                    \"label\": resultado_analise.get('label', 'Erro'),\n",
        "                    \"evidence\": resultado_analise.get('evidence', ''),\n",
        "                    \"summary_id\": item['summary_id'],\n",
        "                    \"claim_id\": i\n",
        "                }\n",
        "                analise_final.append(resultado_final_claim)\n",
        "\n",
        "        print(f\"Análise de '{item['doc_name']}' concluída.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERRO: Arquivo não encontrado - {e}. Verifique se fez o upload do arquivo para o Colab.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO inesperado ao processar {item['doc_name']}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# Salva o resultado final em um único arquivo JSON\n",
        "if analise_final:\n",
        "    caminho_saida_json = \"analise_reivindicacoes.json\"\n",
        "    with open(caminho_saida_json, 'w', encoding='utf-8') as f:\n",
        "        json.dump(analise_final, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"\\n--- ANÁLISE COMPLETA! ---\")\n",
        "    print(f\"Resultado salvo em: {caminho_saida_json}\")\n",
        "\n",
        "    # Imprime o JSON final formatado\n",
        "    print(\"\\nConteúdo do JSON gerado:\")\n",
        "    print(json.dumps(analise_final, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5HAZiLGqLnB",
        "outputId": "72a33efd-aeb4-443b-edaa-b2816177d832"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a análise dos documentos com a nova estratégia...\n",
            "\n",
            "--- Processando: Acórdão 733 de 2025 Plenário ---\n",
            "Criando índice de busca para o documento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice criado com sucesso.\n",
            "Encontradas 3 reivindicações válidas no resumo.\n",
            "Analisando reivindicação 1/3: 'O processo TC 004.980/2017-4 foi iniciado por inic...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando reivindicação 2/3: 'A representação TC 004.980/2017-4, apresentada pel...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando reivindicação 3/3: 'O Acórdão 733/2025 trata do pedido do TCU para que...'\n",
            "ERRO: Bloco JSON não encontrado na saída do modelo para a reivindicação: 'O Acórdão 733/2025 trata do pedido do TCU para que o BNDES seja transformado em autarquia, a fim de alinhar sua estrutura ao controle da União. A decisão final determinou o reconhecimento da dependência do banco e o bloqueio do pagamento de bônus futuros, sob pena de nulidade. Ficou evidente que os lucros do banco decorrem majoritariamente de sua competitividade de mercado, o que justifica equiparação aos bancos privados e não aplicação do teto.'\n",
            "Saída do modelo: {\n",
            "  \"label\": \"Incorreta\",\n",
            "  \"evidence\": \"O Acórdão 733/2025 trata do pedido do TCU para que o BNDES seja transformado em autarquia, a fim de alinhar sua estrutura ao controle da União. A decisão final determinou o reconhecimento da dependência do banco e o bloqueio do pagamento de bônus futuros, sob pena de nulidade. No entanto, a reivindicação de que os lucros do banco decorrem majoritariamente de sua competitividade de mercado e justificam a equiparação aos bancos privados e não aplicação do teto, não é suportada pelo contexto fornecido. A composição e as características do passivo do BNDES podem mudar consideravelmente ao longo do tempo, a depender da diretriz estratégica dada pelo Governo Federal para a atuação do banco, como observado nos últimos anos com a diminuição do passivo com o Tesouro e a mudança da TJLP para a TLP. Dada essa possibilidade de mudança, o que se espera é que o BNDE\n",
            "Análise de 'Acórdão 733 de 2025 Plenário' concluída.\n",
            "\n",
            "--- Processando: Acórdão 764 de 2025 Plenário ---\n",
            "Criando índice de busca para o documento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice criado com sucesso.\n",
            "Encontradas 3 reivindicações válidas no resumo.\n",
            "Analisando reivindicação 1/3: 'O processo TC 024.887/2024-2 foi instaurado pelo M...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando reivindicação 2/3: 'O processo TC 024.887/2024-2 refere-se a questiona...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando reivindicação 3/3: 'No Acórdão 764/2025, o Tribunal de Contas da União...'\n",
            "Análise de 'Acórdão 764 de 2025 Plenário' concluída.\n",
            "\n",
            "--- ANÁLISE COMPLETA! ---\n",
            "Resultado salvo em: analise_reivindicacoes.json\n",
            "\n",
            "Conteúdo do JSON gerado:\n",
            "[\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 733 de 2025 Plenário\",\n",
            "        \"claim_text\": \"O processo TC 004.980/2017-4 foi iniciado por iniciativa do próprio BNDES, visando a regularização de sua política salarial diante das novas diretrizes do Ministério da Fazenda. A principal conclusão do TCU foi o reconhecimento de que o BNDES é uma estatal dependente da União, o que implica a imediata aplicação do teto remuneratório constitucional a todos os seus empregados, inclusive nas subsidiárias. O acórdão também determinou que o banco restituísse os valores recebidos a título de PLR nos últimos cinco anos por violar os limites legais de remuneração.\",\n",
            "        \"label\": \"Correta\",\n",
            "        \"evidence\": \"O processo TC 004.980/2017-4 confirmou que o BNDES é uma estatal dependente da União, o que obrigaria a aplicação do teto remuneratório constitucional a todos os seus empregados, incluindo nas subsidiárias. O acórdão também determinou que o banco devolvesse os valores recebidos a título de PLR nos últimos cinco anos por violar os limites legais de remuneração, de acordo com as diretrizes do Ministério da Fazenda.\",\n",
            "        \"summary_id\": 1,\n",
            "        \"claim_id\": 0\n",
            "    },\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 733 de 2025 Plenário\",\n",
            "        \"claim_text\": \"A representação TC 004.980/2017-4, apresentada pela AGU, questionava a atuação do BNDES como empresa de fomento. Segundo o TCU, embora o banco utilize recursos públicos, como os do FAT e FMM, isso não o caracteriza como dependente da União, pois parte dos recursos provém de arrecadação própria. No entanto, o tribunal decidiu que todos os benefícios pagos aos empregados, como vale-transporte e PLR, deveriam ser imediatamente cortados, mesmo os já homologados pela SEST.\",\n",
            "        \"label\": \"Incorreta\",\n",
            "        \"evidence\": \"De acordo com a decisão do TCU, a representação TC 004.980/2017-4, apresentada pela AGU, questionava a atuação do BNDES como empresa de fomento. O TCU determinou que, embora o banco utilize recursos públicos, como os do FAT e FMM, isso não o caracteriza como dependente da União, pois parte dos recursos provém de arrecadação própria. No entanto, o tribunal decidiu que todos os benefícios pagos aos empregados, como vale-transporte e PLR, deveriam ser imediatamente cortados, mesmo os já homologados pela SEST.\",\n",
            "        \"summary_id\": 1,\n",
            "        \"claim_id\": 1\n",
            "    },\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 733 de 2025 Plenário\",\n",
            "        \"claim_text\": \"O Acórdão 733/2025 trata do pedido do TCU para que o BNDES seja transformado em autarquia, a fim de alinhar sua estrutura ao controle da União. A decisão final determinou o reconhecimento da dependência do banco e o bloqueio do pagamento de bônus futuros, sob pena de nulidade. Ficou evidente que os lucros do banco decorrem majoritariamente de sua competitividade de mercado, o que justifica equiparação aos bancos privados e não aplicação do teto.\",\n",
            "        \"label\": \"Erro\",\n",
            "        \"evidence\": \"Bloco JSON não encontrado na resposta do modelo.\",\n",
            "        \"summary_id\": 1,\n",
            "        \"claim_id\": 2\n",
            "    },\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 764 de 2025 Plenário\",\n",
            "        \"claim_text\": \"O processo TC 024.887/2024-2 foi instaurado pelo Ministério Público junto ao TCU para apurar suposta fraude em licitação no Crea/SP. O objeto da representação era um contrato de aquisição de equipamentos de informática sem licitação, o que motivou o pedido de medida cautelar para suspender o contrato. O TCU, diante da gravidade dos fatos, concedeu a cautelar e determinou a anulação imediata do contrato, identificando direcionamento explícito para a empresa Dell. A decisão foi unânime e com base no Acórdão 1973/2020.\",\n",
            "        \"label\": \"Correta\",\n",
            "        \"evidence\": \"O processo TC 024.887/2024-2 foi instaurado pelo Ministério Público junto ao TCU para apurar suposta fraude em licitação no Crea/SP. O objeto da representação era um contrato de aquisição de equipamentos de informática sem licitação, o que motivou o pedido de medida cautelar para suspender o contrato. O TCU, diante da gravidade dos fatos, concedeu a cautelar e determinou a anulação imediata do contrato, identificando direcionamento explícito para a empresa Dell. A decisão foi unânime e com base no Acórdão 1973/2020.\",\n",
            "        \"summary_id\": 2,\n",
            "        \"claim_id\": 0\n",
            "    },\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 764 de 2025 Plenário\",\n",
            "        \"claim_text\": \"O processo TC 024.887/2024-2 refere-se a questionamentos sobre um contrato firmado entre o Crea/SP e a empresa HP para fornecimento de notebooks por meio de dispensa de licitação. A empresa Convex alegou que os preços estavam acima da média de mercado, mas o TCU concluiu que todas as exigências técnicas eram justificáveis e que a contratação deveria prosseguir normalmente, inclusive com possibilidade de prorrogação. Não houve qualquer recomendação ou ciência determinada ao órgão.\",\n",
            "        \"label\": \"Correta\",\n",
            "        \"evidence\": \"O TCU avaliou as exigências técnicas e concluiu que a contratação com a HP estava justificada, com preços aceitáveis na média do mercado. A empresa Convex apresentou questionamentos, mas o TCU rejeitou, pois as exigências técnicas eram justificáveis e a contratação deveria prosseguir normalmente, inclusive com possibilidade de prorrogação.\",\n",
            "        \"summary_id\": 2,\n",
            "        \"claim_id\": 1\n",
            "    },\n",
            "    {\n",
            "        \"doc_name\": \"Acórdão 764 de 2025 Plenário\",\n",
            "        \"claim_text\": \"No Acórdão 764/2025, o Tribunal de Contas da União analisou um caso envolvendo a compra direta de equipamentos pelo Crea/SP, tendo sido identificado superfaturamento e ausência de pesquisa de preços. O TCU concluiu que houve dano ao erário, determinou o ressarcimento imediato pela empresa contratada e recomendou ao Crea/SP a adoção de software livre para futuras contratações, evitando a dependência de fornecedores específicos.\",\n",
            "        \"label\": \"Correta\",\n",
            "        \"evidence\": \"O superfaturamento e ausência de pesquisa de preços identificados no Acórdão 764/2025 do TCU indicam dano ao erário. No entanto, neste caso, o processo de licitação foi conduzido de acordo com a Instrução Normativa 65/2021, e a participação de empresas representantes de outros fabricantes e as cotações recebidas na fase de orçamentação indicam a possibilidade de atendimento dos requisitos por outros fabricantes, além da locação de equipamentos tecnológicos ser a solução mais adequada para atender às necessidades do Conselho, eliminando a necessidade de aquisição de bens patrimoniais e os custos associados à gestão de bens obsoletos. O TCU recomendou o uso de software livre para futuras contratações, mas neste caso, a empresa vencedora ofertou o menor lance, indicando a eficiência da decisão tomada.\",\n",
            "        \"summary_id\": 2,\n",
            "        \"claim_id\": 2\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    }
  ]
}